{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.cluster import KMeans\n",
    "import reverse_geocoder as rg\n",
    "import csv\n",
    "import os\n",
    "import pgeocode\n",
    "from geopy.geocoders import Nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input(folderpath, filename, delimiter = '|', keep_columns = [], key_column = ''):    \n",
    "    with open(folderpath + filename, \"r\", encoding = 'utf-8-sig') as csvfile:\n",
    "        dict_reader = csv.DictReader(csvfile, delimiter = delimiter)\n",
    "\n",
    "        data = {}\n",
    "        for row in dict_reader:\n",
    "            d = {}\n",
    "            for k in list(row.keys()):\n",
    "                if k in keep_columns:\n",
    "                    d[k] = row[k].strip()\n",
    "            \n",
    "            data[row[key_column].strip()] = d\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing transactions...\n",
      "processing updates FraudReport-JanMar2023.csv ...\n",
      "processing updates FraudReport-JulSep2023.csv ...\n",
      "processing updates FraudReport-OctDec2023.csv ...\n",
      "processing updates FraudReport-AprJun2023.csv ...\n",
      "loaded 1906687 transactions and 1850921 updates\n",
      "Missing 76781 out of 1906687\n"
     ]
    }
   ],
   "source": [
    "INPUT_DATA_PATH = './files/uniteller/'\n",
    "\n",
    "all_data = {}\n",
    "data_updates = {}\n",
    "missing = {}\n",
    "\n",
    "print(\"Processing transactions...\")\n",
    "for f in os.listdir(INPUT_DATA_PATH):\n",
    "    if f.endswith('.csv') and 'FraudReport_' in f:\n",
    "        \n",
    "        data = process_input(INPUT_DATA_PATH, f, delimiter = '|', \n",
    "            keep_columns = ['txIdentifier', 'txOriginCountry', 'txAgentState', 'paymentCountry', 'paymentAmount', 'txCreationDateLocal'], \n",
    "            key_column = 'txIdentifier')\n",
    "        all_data.update(data)\n",
    "\n",
    "for f in os.listdir(INPUT_DATA_PATH):\n",
    "    if f.endswith('.csv') and 'FraudReport-' in f:\n",
    "        print(\"processing updates\", f, '...')\n",
    "        data = process_input(INPUT_DATA_PATH, f, delimiter = ',', \n",
    "            keep_columns = ['txIdentifier', 'txOriginCountry', 'txAgentState', 'paymentCountry', 'paymentAmount', 'txCreationDateLocal', 'txAmount(USD)'], \n",
    "            key_column = 'txIdentifier')\n",
    "        data_updates.update(data)\n",
    "\n",
    "print(f'loaded {len(all_data)} transactions and {len(data_updates)} updates')\n",
    "for d in all_data:\n",
    "    if d in data_updates:\n",
    "        all_data[d].update(data_updates[d])\n",
    "    else:\n",
    "        missing[d] = all_data[d]\n",
    "\n",
    "print(f'Missing {len(missing)} out of {len(all_data)}')\n",
    "# txOriginCountry, paymentCountry, paymentAmount, txCreationDateLocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_date(date):\n",
    "    s = date.split(' ')[0]\n",
    "    s = ''.join(list(reversed(s.split('/'))))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_data = {k: v for k, v in all_data.items() if k not in missing}\n",
    "sorted_by_date = sorted(updated_data, key = lambda x: split_date(updated_data[x]['txCreationDateLocal']))\n",
    "\n",
    "GEOCODE_CACHE = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out_data = {}\n",
    "out_data_by_date = {}\n",
    "i = 0\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"fnunigeo\")\n",
    "\n",
    "for k in sorted_by_date:\n",
    "    data = {}\n",
    "    data['amount'] = updated_data[k]['txAmount(USD)']\n",
    "    data['order_date'] = updated_data[k]['txCreationDateLocal'].split(' ')[0]\n",
    "    data['from_country'] = updated_data[k]['txOriginCountry']\n",
    "    data['from_state'] = updated_data[k]['txAgentState']\n",
    "    data['to_country'] = updated_data[k]['paymentCountry']\n",
    "\n",
    "    if len(out_data_by_date.get(i, [])) == 0:\n",
    "        out_data_by_date[i] = [data]\n",
    "    elif out_data_by_date[i][0]['order_date'] == data['order_date']:\n",
    "        out_data_by_date[i].append(data)\n",
    "    else:\n",
    "        i += 1\n",
    "        if i > 31:\n",
    "            break\n",
    "        out_data_by_date[i] = [data]\n",
    "\n",
    "# Aggregating data by same from and to countries\n",
    "for k in out_data_by_date:\n",
    "    data = out_data_by_date[k]\n",
    "    data_by_countries = {}\n",
    "\n",
    "    out_data[k] = []\n",
    "\n",
    "    for d in data:\n",
    "        key = f\"{d['from_state']}_{d['to_country']}\"\n",
    "        if key not in data_by_countries:\n",
    "            data_by_countries[key] = {\n",
    "                'amount': round(float(d['amount']), 2),\n",
    "                'count': 1\n",
    "            }\n",
    "        else:\n",
    "            data_by_countries[key]['amount'] += round(float(d['amount']), 2)\n",
    "            data_by_countries[key]['count'] += 1\n",
    "\n",
    "    for key in data_by_countries:\n",
    "        d = data_by_countries[key]\n",
    "        from_country, to_country = key.split('_')\n",
    "\n",
    "        if from_country not in GEOCODE_CACHE:\n",
    "            GEOCODE_CACHE[from_country] = geolocator.geocode(from_country)\n",
    "\n",
    "        if to_country not in GEOCODE_CACHE:\n",
    "            GEOCODE_CACHE[to_country] = geolocator.geocode(to_country)\n",
    "        \n",
    "        out_data[k].append({\n",
    "            'from_country': from_country,\n",
    "            'to_country': to_country,\n",
    "            'total_amount': d['amount'],\n",
    "            'total_count': d['count'],\n",
    "            'order_date': data[0]['order_date'],\n",
    "            'from_lat': GEOCODE_CACHE[from_country].latitude,\n",
    "            'from_lon': GEOCODE_CACHE[from_country].longitude,\n",
    "            'to_lat': GEOCODE_CACHE[to_country].latitude,\n",
    "            'to_lon': GEOCODE_CACHE[to_country].longitude\n",
    "        })\n",
    "\n",
    "with open('files/transactions_uniteller.json', 'w') as f:\n",
    "    json.dump(out_data, f, indent = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'txIdentifier': '702531449644',\n",
       " 'paymentAmount': '27088',\n",
       " 'paymentCountry': 'MEXICO',\n",
       " 'txAgentState': 'CALIFORNIA',\n",
       " 'txOriginCountry': 'USA',\n",
       " 'txCreationDateLocal': '27/12/2023 23:58',\n",
       " 'txAmount(USD)': '1600'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_data[sorted_by_date[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20231227'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_date(updated_data[sorted_by_date[-1]]['txCreationDateLocal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
